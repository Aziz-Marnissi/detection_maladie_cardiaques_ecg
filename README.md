# ü´Ä ECG Heartbeat Classification System

> Syst√®me professionnel d'analyse et de classification des battements cardiaques MIT-BIH ‚Äî Machine Learning meets Cardiologie.

---

## üìã Table des mati√®res

- [Aper√ßu](#-aper√ßu)
- [Architecture du projet](#-architecture-du-projet)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Classes de battements](#-classes-de-battements)
- [Mod√®les disponibles](#-mod√®les-disponibles)
- [Pipeline de traitement](#-pipeline-de-traitement)
- [R√©sultats](#-r√©sultats)
- [Configuration](#-configuration)

---

## üî¨ Aper√ßu

Ce syst√®me prend en entr√©e des enregistrements ECG bruts de la base **MIT-BIH Arrhythmia Database** et produit en sortie une classification automatique des battements cardiaques en 5 types, comparant plusieurs approches ML et Deep Learning.

Le pipeline est enti√®rement automatis√© : t√©l√©chargement ‚Üí nettoyage ‚Üí extraction ‚Üí entra√Ænement ‚Üí rapport.

**Ce que √ßa fait concr√®tement :**

- T√©l√©charge et lit les enregistrements MIT-BIH via `wfdb`
- Nettoie le signal ECG (NeuroKit2 ou fallback brut)
- D√©tecte les R-peaks avec 3 m√©thodes en cascade (XQRS ‚Üí GQRS ‚Üí fallback)
- Extrait des segments de 300 points centr√©s sur chaque battement
- Entra√Æne 4 mod√®les ML classiques + 6 architectures Deep Learning
- G√©n√®re un rapport complet avec matrices de confusion, courbes ROC et comparaison

---

## üóÇÔ∏è Architecture du projet

```
ecg-classification/
‚îÇ
‚îú‚îÄ‚îÄ ecg_pipeline.py       ‚Üê script principal
‚îú‚îÄ‚îÄ ecg_analysis.log      ‚Üê logs auto-g√©n√©r√©s
‚îÇ
‚îú‚îÄ‚îÄ mitdb/                ‚Üê enregistrements MIT-BIH
‚îú‚îÄ‚îÄ cache/                ‚Üê cache disque (hash MD5)
‚îú‚îÄ‚îÄ models/               ‚Üê mod√®les sauvegard√©s
‚îú‚îÄ‚îÄ plots/                ‚Üê visualisations
‚îî‚îÄ‚îÄ report/               ‚Üê rapport final
    ‚îú‚îÄ‚îÄ performance_summary.csv
    ‚îú‚îÄ‚îÄ cm_*.png
    ‚îú‚îÄ‚îÄ roc_curves.png
    ‚îî‚îÄ‚îÄ model_comparison.png
```

Le code est organis√© en composants bien s√©par√©s :

```
ECGConfig         ‚Üí  tous les param√®tres au m√™me endroit
CacheManager      ‚Üí  cache m√©moire + disque, √©vite les recalculs
ECGDataLoader     ‚Üí  t√©l√©chargement, lecture, nettoyage du signal
FeatureExtractor  ‚Üí  segments + features morphologiques & temporelles
SklearnModel      ‚Üí  wrapper RF / XGBoost / Logistic / SVM
CNNModel          ‚Üí  CNN 1D TensorFlow/Keras
ECGPipeline       ‚Üí  orchestrateur principal
```

---

## ‚öôÔ∏è Installation

```bash
# Cloner le d√©p√¥t
git clone https://github.com/votre-utilisateur/ecg-classification.git
cd ecg-classification

# Environnement virtuel
python -m venv venv
source venv/bin/activate        # Linux/macOS
# venv\Scripts\activate         # Windows

# D√©pendances minimales
pip install numpy pandas scipy scikit-learn matplotlib seaborn wfdb

# D√©pendances compl√®tes (recommand√©)
pip install tensorflow xgboost neurokit2 librosa joblib
```

> Les d√©pendances optionnelles (TensorFlow, XGBoost, NeuroKit2) sont d√©tect√©es automatiquement. Si elles sont absentes, le syst√®me bascule sur des m√©thodes de fallback sans planter.

---

## üöÄ Utilisation

### Lancement rapide

```bash
python ecg_pipeline.py
```

### Via l'API Python

```python
from ecg_pipeline import ECGConfig, ECGPipeline

config = ECGConfig(max_epochs=100, batch_size=64, cache_enabled=True)
pipeline = ECGPipeline(config)

# Charger et traiter les donn√©es
pipeline.load_data(["100", "101", "102"], download=True)
pipeline.extract_features()

# Entra√Æner
ml_data = pipeline.prepare_ml_data()
pipeline.train_classical_models(*ml_data)

dl_data = pipeline.prepare_dl_data()
pipeline.train_cnn(*dl_data)

# G√©n√©rer le rapport
pipeline.generate_report(output_dir="report")
```

### Entra√Æner un seul mod√®le

```python
from ecg_pipeline import SklearnModel, ECGConfig

model = SklearnModel(model_type="rf", config=ECGConfig())
model.train(X_train, y_train, class_weights=weights)

results = model.evaluate(X_test, y_test, label_encoder)
print(f"F1  :  {results['f1_score']:.3f}")
print(f"Acc :  {results['accuracy']:.3f}")

model.save("models/rf.joblib")
```

Les types de mod√®les disponibles sont `'rf'`, `'xgboost'`, `'logistic'` et `'svm'`.

---

## üè∑Ô∏è Classes de battements

5 types de battements selon la nomenclature MIT-BIH :

| Label | Nom complet | Description |
|:---:|---|---|
| `N` | Normal | Battement sinusal standard, pic R net, ligne de base stable |
| `V` | Ventricular Ectopic | Contraction ventriculaire pr√©matur√©e, morphologie invers√©e |
| `A` | Atrial Ectopic | Contraction auriculaire pr√©matur√©e, intervalle RR raccourci |
| `L` | Left Bundle Branch Block | Bloc de branche gauche, QRS √©largi |
| `R` | Right Bundle Branch Block | Bloc de branche droite, aspect rSR' caract√©ristique |

Le dataset est naturellement tr√®s d√©s√©quilibr√© (N est de loin la classe majoritaire). Le syst√®me g√®re √ßa automatiquement via `class_weight='balanced'` pour les mod√®les sklearn, et `sample_weight` pour XGBoost.

---

## ü§ñ Mod√®les disponibles

### Machine Learning classique

| Mod√®le | Config notable |
|---|---|
| **Random Forest** | 200 arbres, profondeur max 15, `class_weight='balanced'` |
| **XGBoost** | 300 estimateurs, lr=0.05, subsample=0.9 |
| **Logistic Regression** | multinomial, 1000 it√©rations max |
| **SVM** | kernel RBF, probabilit√©s activ√©es |

### Deep Learning ‚Äî CNN 1D

L'architecture de base est un CNN 1D √† 3 blocs convolutifs :

```
Input (300, 1)
  ‚Üì
Conv1D(64, kernel=7)  ‚Üí  BatchNorm  ‚Üí  MaxPool  ‚Üí  Dropout(0.3)
  ‚Üì
Conv1D(128, kernel=5) ‚Üí  BatchNorm  ‚Üí  MaxPool  ‚Üí  Dropout(0.3)
  ‚Üì
Conv1D(256, kernel=3) ‚Üí  BatchNorm  ‚Üí  MaxPool  ‚Üí  Dropout(0.4)
  ‚Üì
GlobalAveragePooling
  ‚Üì
Dense(256, relu, L2)  ‚Üí  Dropout(0.5)
  ‚Üì
Dense(n_classes, softmax)
```

Optimizer Adam (lr=0.001), perte Sparse Categorical Crossentropy.

Deux callbacks guident l'entra√Ænement : **EarlyStopping** (patience=15, restaure les meilleurs poids) et **ReduceLROnPlateau** (patience=5, divise le lr par 2).

### Variantes exp√©riment√©es

| Mod√®le | Sp√©cificit√© |
|---|---|
| **CNN++** | Architecture CNN plus profonde |
| **CNN 2D** | Convolutions 2D sur repr√©sentation temps-fr√©quence |
| **CNN Am√©lior√©** | Connexions r√©siduelles |
| **CNN+LSTM** | CNN pour l'extraction locale, LSTM pour les d√©pendances temporelles |
| **Hybride** | Fusion features morphologiques + segments bruts |

---

## üîÑ Pipeline de traitement

```
Enregistrements MIT-BIH
        ‚îÇ
        ‚ñº
  T√©l√©chargement wfdb  (retry x3 automatique)
        ‚îÇ
        ‚ñº
  Nettoyage signal  (NeuroKit2 ‚Üí fallback brut)
        ‚îÇ
        ‚ñº
  D√©tection R-peaks  (XQRS ‚Üí GQRS ‚Üí find_peaks)
        ‚îÇ
        ‚ñº
  Association annotations  (tol√©rance 100ms)
        ‚îÇ
        ‚ñº
  Extraction segments  (200ms avant + 400ms apr√®s le R-peak)
        ‚îÇ
        ‚ñº
  R√©√©chantillonnage  ‚Üí  300 points par segment
        ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                           ‚ñº
  Features morpho               Segments bruts
  (stats, FFT, skewness)        (pour CNN)
        ‚îÇ                           ‚îÇ
        ‚ñº                           ‚ñº
  Train / Val / Test split    Train / Val / Test split
        ‚îÇ                           ‚îÇ
        ‚ñº                           ‚ñº
  StandardScaler              Reshape  ‚Üí  (N, 300, 1)
        ‚îÇ                           ‚îÇ
        ‚ñº                           ‚ñº
  RF / XGBoost / LR / SVM     CNN / CNN++ / CNN+LSTM...
        ‚îÇ                           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚ñº
               Rapport de performance
```

---

## üìä R√©sultats

### Ce qu'on observe sur les matrices de confusion

Tous les mod√®les souffrent du m√™me probl√®me : le fort d√©s√©quilibre de classes (448 N contre seulement 7 A dans le jeu de test) pousse les mod√®les √† favoriser la classe majoritaire.

**Deep Learning ‚Äî bilan par mod√®le :**

| Mod√®le | Rappel sur A | Rappel sur N | Verdict |
|---|:---:|:---:|---|
| CNN baseline | 100% ‚úÖ | 0% ‚ùå | biais total vers A |
| CNN++ | 71% ‚úÖ | 99.8% ‚úÖ | meilleur √©quilibre |
| CNN 2D | 0% ‚ùå | 100% ‚úÖ | ignore totalement A |
| CNN Am√©lior√© | 0% ‚ùå | 100% ‚úÖ | ignore totalement A |
| CNN+LSTM | 14% ‚ö†Ô∏è | 84% ‚ö†Ô∏è | plus de faux positifs |
| Hybride | 100% ‚úÖ | 99.8% ‚úÖ | proche du CNN++ |

**ML classique :** Random Forest, Logistic Regression, SVM et XGBoost donnent tous le m√™me r√©sultat : 7/7 sur A et 447/448 sur N ‚Äî performances solides et stables.

### Courbes d'apprentissage CNN

La training loss descend rapidement mais la validation loss remonte d√®s la 2e epoch ‚Äî signe clair d'overfitting. La validation accuracy reste proche de 0 : le mod√®le pr√©dit quasi-exclusivement N sur le jeu de validation.

### Courbe ROC

AUC = 0.42 sur les deux classes, ce qui est en dessous d'un classifieur al√©atoire. Ce chiffre confirme que le mod√®le n'a pas appris de repr√©sentation utile pour s√©parer les classes, faute de donn√©es suffisantes sur les classes minoritaires.

### Pistes d'am√©lioration

- **SMOTE / ADASYN** ‚Äî r√©√©chantillonner les classes minoritaires avant l'entra√Ænement
- **Focal Loss** ‚Äî p√©naliser davantage les erreurs sur les classes rares
- **Plus de donn√©es** ‚Äî utiliser les 48 enregistrements complets (10 charg√©s par d√©faut)
- **Data augmentation** ‚Äî bruit gaussien et time-stretching sur les segments
- **Ensemble** ‚Äî combiner CNN++ et Random Forest par stacking

---

## üõ†Ô∏è Configuration

Tout est centralis√© dans `ECGConfig`, aucun param√®tre √©parpill√© dans le code :

```python
@dataclass
class ECGConfig:
    # Signal
    fs: int = 360                     # fr√©quence d'√©chantillonnage (Hz)
    window_before_ms: float = 200.0   # fen√™tre avant le R-peak
    window_after_ms: float = 400.0    # fen√™tre apr√®s le R-peak
    desired_length: int = 300         # longueur cible apr√®s r√©√©chantillonnage

    # Classes
    valid_labels: List[str] = ["N", "V", "A", "L", "R"]

    # Splits
    test_size: float = 0.2
    val_size: float = 0.2
    random_state: int = 42

    # Deep Learning
    max_epochs: int = 100
    batch_size: int = 32
    early_stopping_patience: int = 15
    reduce_lr_patience: int = 5
    min_lr: float = 1e-7

    # Syst√®me
    cache_enabled: bool = True
    n_workers: int = 4
```

---

## üìÑ Licence

MIT ‚Äî Donn√©es MIT-BIH disponibles sur [PhysioNet](https://physionet.org/content/mitdb/1.0.0/).

---

> Pour de meilleures performances : charger les 48 enregistrements complets, utiliser un GPU pour TensorFlow, et corriger le d√©s√©quilibre de classes avec SMOTE avant d'entra√Æner les mod√®les Deep Learning.
